% !TEX root = ../main.tex

% Background section

\section{Background}

Inequalities in statistics provide a means of bounding measures and quantities. 
They are usually used to specify bounds on quantities when these bounds are particularly difficult or intractable to compute.
Inequalities play an important role in the algorithm of statistical learning and machine learning. 
They are involved in underpinning methods or approaches used in actual cases.

Here are several famous concentration inequalities involved in statistical learning. 
\begin{itemize}
  \item Markov's Inequality
  \item Chebyshev's Inequality 
  \item Bounded Differences Inequality 
\end{itemize}

In this project, I will focus on the Bounded Differences Inequality.

\subsection{Chebyshev's Inequality}
Chebyshev's Inequality is a basic tool to develop the bounded inequalities for different quantities. 
Usually it is stated for random variables, while sometimes it can be generalized for measure spaces.
Here is the probalistic statement.
\begin{theorem} 
  For some $a \in X$ where $X \subseteq R$, let $f$ be a non-negative function such that
  $\{ f(x) \geq b, for all x \geq a\}$, where $b \in Y$ where $Y \subseteq R$. Then the following ineuqality holds,
  \begin{equation}
    P(x \geq a) \leq \frac{\mathbb{E}f(x)}{b}
  \end{equation}
\end{theorem}

\subsection{Chernoff's bound}
The Chernoff's bound is a version of Markov's inequality, as well as the Chebyshev's one.

Suppose the function $f$ is monotonically increasing.
Thus, for every $x \geq a$, $f(x) \geq f(a)$. 
Substitute $b = f(a)$ in Chebyshev's Inequality, and we have:

\begin{equation}
  P(x \geq a) \leq \frac{\mathbb{E}f(x)}{f(a)}
\end{equation}

Markov's Inequality says that

\begin{equation}
  P(x \geq a) \leq \frac{\mathbb{E}x}{a}
\end{equation}

It holds for $a >0$ and nonnegetive x, 

\begin{equation}
  P(|x-\mathbb{E}(x)| \geq a) \leq \frac{\mathbb{E}\left\{|x-\mathbb{E}(x)|^{2}\right\}}{a^{2}}=\frac{\operatorname{Var}\{x\}}{a^{2}}
\end{equation}

The Chernoff's bound is then,

\begin{equation}
  P(x \geq a) \leq \frac{\mathbb{E}e^{sx}}{e^{sa}}
\end{equation}

\subsection{Pre-Gaussian Random Variables}
The Pre-Gaussian random variables develop from the Sub-Gaussian random variable.
The Sub-Gaussian random variables are defined as 

\begin{equation}
  \log M_{X-\mu}(\lambda) \leq \frac{\lambda^{2} \sigma^{2}}{2}
\end{equation}

The Pre-Gaussian has one more parameter than the Sub-Gaussian one.

\begin{definition} \cite*{Bartlett:2020}
  $X$ is pre-Gaussian with parameters $(\sigma^2,b)$ if, for all $\left| \lambda \right| < 1/b$,
  \begin{equation}
    \log M_{X-\mu}(\lambda) \leq \frac{\lambda^{2} \sigma^{2}}{2}
  \end{equation}
\end{definition}

Example:

\begin{itemize}
  \item Sub-Gaussian $X$ with parameter $\sigma^2$ is pre-Gaussian with parameters $(\sigma^2, b)$ for all $b>0$.
\end{itemize}

\subsection{Martingale Difference Sequences}

The martingales have the following definition.

\begin{definition} \cite*{Bartlett:2020}
  A sequence $Y_n$ of random variables adapted to a filtration $\mathcal{F}_n$ is a martingale if, for all $n$,
  \begin{equation}
    \begin{aligned}
    \mathbb{E}\left|Y_{n}\right| &<\infty \\
    \mathbb{E}\left[Y_{n+1} \mid \mathcal{F}_{n}\right] &=Y_{n}
    \end{aligned}
  \end{equation}
\end{definition}

Note:

\begin{itemize}
  \item $\mathcal{F}_{n}$ is a filtration means these $\sigma$-fields are nested: $\mathcal{F}_{n} \subset \mathcal{F}_{n+1}$.
  \item $Y_n$ is adapted to $\mathcal{F}_{n}$ means  that each $Y_n$ is measurable with respect to $\mathcal{F}_n$.
\end{itemize}

Then the martingale difference sequencs can be generated based on the definition of martingales.

\begin{definition} \cite*{Bartlett:2020}
  A sequence $D_n$ of random variables adapted to a filtration $\mathcal{F}_n$ is a martingale Difference sequence if, for all n,
  \begin{equation}
    \begin{aligned}
    \mathbb{E}\left|D_{n}\right| &<\infty \\
    \mathbb{E}\left[D_{n+1} \mid \mathcal{F}_{n}\right] &=0
    \end{aligned}
  \end{equation}
\end{definition}

% ...